<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.152.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hristo Papazov - Academic Website</title>
    <meta name="description" content="Academic Research Website">
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>
<body>
    
    <nav id="menu">
        
        <a href="/" >Home</a>
        
    </nav>

    
    <main id="main">
        
<h1>Hristo Papazov</h1>

<div class="bio">
<img src="/images/oeschinensee.jpg" alt="Hristo Papazov" style="float: left; margin-right: 20px; margin-bottom: 10px; max-width: 275px; border-radius: 5px;">
<div class="social-icons" style="float: left; clear: left; margin-right: 20px; width: 275px; text-align: center;">
<a href="https://scholar.google.com/citations?user=o7ILrm4AAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
<a href="https://www.linkedin.com/in/hristo-papazov/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://github.com/fanagor" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://bsky.app/profile/fanagor.bsky.social" target="_blank" title="Bluesky"><i class="fab fa-twitter"></i></a>
</div>
<p>I am a fourth-year PhD student in the <a href="https://www.epfl.ch/labs/tml/">Theory of Machine Learning</a> lab at <a href="https://www.epfl.ch/en/">EPFLðŸ‡¨ðŸ‡­</a>, where I am fortunate to be advised by <a href="https://people.epfl.ch/nicolas.flammarion">Nicolas Flammarion</a>. My research focuses on uncovering the hidden algorithmic processes underlying structured data through discrete and gradient-based methods.</p>
<!-- I believe that gradient-based loss minimization can only learn System I tasks, and that mastering System II intelligence also requires discrete approaches. -->
<p>Before joining EPFL, I spent a year as a PhD student in the math department at <a href="https://www.mit.edu/">MIT</a>, working on discrete algorithms. Prior to that, I completed a bachelorâ€™s degree in math at <a href="https://www.princeton.edu/">Princeton</a>, where I worked under the supervision of <a href="https://math.mit.edu/directory/profile.html?pid=2579">Sasha Logunov</a> and <a href="https://web.math.princeton.edu/~naor/">Assaf Naor</a>.</p>
<p>For more information, consider my full <a href="">CV</a>.</p>
<div style="clear: both;"></div>
</div>
<h2 id="research-interests">Research Interests</h2>
<ul>
<li>Algorithmic Learning Theory</li>
<li>Program Synthesis</li>
<li>Algorithmic Information Theory</li>
<li>Grammatical Inference</li>
</ul>
<h2 id="contact">Contact</h2>
<p><strong>Email:</strong> <a href="mailto:hristo.papazov@epfl.ch">hristo.papazov@epfl.ch</a></p>
<h2 id="selected-publications">Selected Publications</h2>
<p>For a full list of publications, please visit my <a href="https://scholar.google.com/citations?user=o7ILrm4AAAAJ&amp;hl=en">Google Scholar</a> page.</p>
<div class="paper">
<div class="paper-title">Learning Algorithms in the Limit</div>
<div class="paper-authors"><strong>Hristo Papazov</strong>, Nicolas Flammarion</div>
<div class="paper-venue">COLT, 2025</div>
<div class="paper-links">
<a href="/papers/Learning Algorithms in the Limit.pdf">[PDF]</a>
<a href="/slides/LAL -- COLT.key">[Slides]</a>
<a href="https://arxiv.org/abs/2506.15543">[arXiv]</a>
</div>
</div>
<div class="paper">
<div class="paper-title">Leveraging Continuous Time to Understand Momentum When
Training Diagonal Linear Networks</div>
<div class="paper-authors"><strong>Hristo Papazov</strong>, Scott Pesme, Nicolas Flammarion</div>
<div class="paper-venue">AISTATS, 2024</div>
<div class="paper-links">
<a href="/papers/Momentum Gradient Flow.pdf">[PDF]</a>
<a href="/posters/Momentum Gradient Flow Poster.pdf">[Poster]</a>
<a href="https://arxiv.org/abs/2403.05293">[arXiv]</a>
</div>
</div>
<div class="paper">
<div class="paper-title">An Elliptic Adaptation of Ideas of Carleman and Domar from Complex Analysis Related to Levinsonâ€™s LogLog Theorem</div>
<div class="paper-authors">Aleksandr Logunov, <strong>Hristo Papazov</strong></div>
<div class="paper-venue">Journal of Mathematical Physics, 2021</div>
<div class="paper-links">
<a href="/papers/LogLog Theorem.pdf">[PDF]</a>
<a href="https://arxiv.org/abs/2012.07169">[arXiv]</a>
</div>
</div>
<hr>
<footer>
  <p>Last updated: {{ .Lastmod.Format "December 18, 1998" }}</p>
</footer>



    </main>
</body>
</html>
